<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
  <script src="js/scroll.js"></script>
</head>

<title>Generalization</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="#Home">Compositional Generalization</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="https://people.csail.mit.edu/lishuang/">Main Page</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="http://accessibility.mit.edu" target="_blank">Accessibility</a>
				</li>
			</ul>
		</div>
	</nav>








	<!-- Publications -->
	<br><br><br><br>
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Mission</h3>

		<p>A key aspect of human intelligence is the ability to learn increasingly complex concepts by synthesizing simple ideas, enabling both rapid learning and adaptation of knowledge. To unlock its full potential, AI must exhibit similar generalization abilities.</p>
			<!-- We aim to teach AI systems to exhibit such generalization ability in different domains.  -->

		<p>My research into Compositional Generalization explores three main paradigms: achieving compositionality 1) by composing concepts and goals, 2) by composing models, and 3) by transferring compositionality</p>
		<!-- We hope our study on generalization will provide a possible solution for future AI systems to continually learn from the world by combining previously learned knowledge and new skills. -->
		

		
		<hr>
	</div>




	<!-- Publications -->
	<br><br>
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
 			<br>

			<small><small>
		  	<font color="black">Research Topics:
		  		<a href="#composeconcept" onclick="showPubs(2)">By Composing Concepts and Goals</a> /
		  		<a href="#composemodel" onclick="showPubs(2)">By Composing Models</a> /
		  		<a href="#transfercompose" onclick="showPubs(2)">By Transferring Compositionality</a> /
		  		<a href="#others" onclick="showPubs(2)">Others</a>
			</font><br>
			</small></small>
		</h3>


		<div id="pubs"></div>



		<script id="pubs_by_topic" language="text">
		
			<hr>

			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		

			
			<div id="composeconcept" style="padding-top: 80px; margin-top: -80px;">
				<h5>By Composing Concepts and Goals</h5>
			</div>
			<br>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/22-compose-duffusion-model/teaser_glide.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Composable Diffusion Models</font></b><br>
				<a href="https://nanliu.io/" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>
				(*equal contribution)
				<br>
				<b><a href="" target="_blank">ECCV 2022</a></b> <br>
				<a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2206.01714.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch" target="_blank"> <small>[Github]</small></a>
				<a href="https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/compose_glide.ipynb" target="_blank"> <small>[Colab]</small></a>
				<a href="https://huggingface.co/spaces/Shuang59/Composable-Diffusion" target="_blank"> <small>[Demo]</small></a>
				<br>
				<a> Press coverage: <a href="https://news.mit.edu/2022/ai-system-makes-models-like-dall-e-2-more-creative-0908" target="_blank">MIT News</a>, <a href="https://www.csail.mit.edu/news/ai-system-makes-models-dall-e-2-more-creative" target="_blank">MIT CSAIL News</a> </a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation.</p>
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
	            		<source src="files/paper/21-neurips-compose-relation/clevr_teaser.mp4" type="video/mp4">
	        		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning to Compose Visual Relations</font></b><br>
				<a href="" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				(*equal contribution)
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://ctrlgenworkshop.github.io/accepted_papers.html" target="_blank">NeurIPS Workshop on Controllable Generative Modeling 2021</a>, <font color="firebrick">Outstanding Paper Award</font> </b> <br>
				<a> Press coverage: <a href="https://news.mit.edu/2021/ai-object-relationships-image-generation-1129" target="_blank">MIT News</a>, <a href="https://www.csail.mit.edu/news/artificial-intelligence-understands-object-relationships" target="_blank">MIT CSAIL News</a> </a> <br>
				<a href="https://composevisualrelations.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2111.09297" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/nanlliu/compose-visual-relations" target="_blank"> <small>[Code]</small></a><br>
				</div>
				<div class="col-md-12">
				<br>
				<p>The visual world around us can be described as a structured set of objects and their associated relations. In this work, we propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decomposition enables our model to effectively understand the underlying relational scene structure.</p>
				</div>

			</div><hr>


			<!--
			<div class="row">
				<div class="col-md-3">
				<img class="img-fluid img-rounded" src="files/paper/21-neurips-decompose/teaser.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Unsupervised Learning of Compositional Energy Concepts</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://www.yash-sharma.com" target="_blank">Yash Sharma</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2021</a></b> <br>
				<a href="https://energy-based-model.github.io/comet/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2111.03042.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yilundu/comet" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> We introduce an approach to decompose images, in an unsupervised manner, into separate component energy functions. 
	            These energy functions can both represent global factors of variation, such as facial expression and hair color, as well
	            as local factors of variations, such as the objects in a scene. Decomposed energy functions generalize well, and may be
	            recombined with energy function discovered by training a separate instance of approach on a different dataset, enabling
	            the recombination of objects and lighting conditions across datasets.</p>
				</div>
			</div><hr>
		-->



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/20-neurips-ebm-compositional/comp-face.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Energy Based Models</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2020</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<a href="https://energy-based-model.github.io/compositional-generation-inference/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2004.06030.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yilundu/ebm_compositionality" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given one distribution for smiling face images, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We showcase the breadth of unique capabilities of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image. </p>
				</div>
			</div><hr>





			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		

			<br>
			<div id="composemodel" style="padding-top: 80px; margin-top: -80px;">
				<h5>By Composing Models</h5>
			</div><br>





			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/22-compose-pretrained-models/teaser-2.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Composing Ensembles of Pre-trained Models via Iterative Consensus</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				(*equal contribution. Shuang Li did all the experiments on image generation, video question answering, and mathematical reasoning. Yilun Du did all the experiments on robot manipulation.)
				<br>
				<b><a href="https://iclr.cc/" target="_blank">ICLR 2023</a></b> <br>
				<a href="https://energy-based-model.github.io/composing-pretrained-models-web/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2210.11522" target="_blank"> <small>[Paper]</small></a>
				<a href="" target="_blank"> <small>[Github]</small></a>
                </div>

				<div class="col-md-12">
				<br>
				<p>Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. We propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as "generators" or "scorers" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation.
				</p>
				</div>

			</div><hr>



	

			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		



			<br>
			<div id="transfercompose" style="padding-top: 80px; margin-top: -80px;">
				<h5>By Transferring Compositionality</h5>
			</div><br>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/21-virtualhome-language/vid1.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Pre-Trained Language Models for Interactive Decision-Making</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="" target="_blank">Chris Paxton</a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://clintonjwang.github.io/" target="_blank">Clinton Wang</a>,
				<a href="" target="_blank">Linxi Fan</a>,
				<a href="https://taochenshh.github.io" target="_blank">Tao Chen</a>,
				<a href="" target="_blank">De-An Huang</a>,
				<a href="" target="_blank">Ekin Akyürek</a>, 
				<a href="" target="_blank">Anima Anandkumar<sup>+</sup></a>,
				<a href="" target="_blank">Jacob Andreas<sup>+</sup></a>,
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch<sup>+</sup></a>, 
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba<sup>+</sup></a>, and
				<a href="" target="_blank">Yuke Zhu<sup>+</sup></a> <br>
				Junior authors are ordered based on their contributions and senior authors<sup>+</sup> are ordered alphabetically
				<br>
				<b><a href="https://nips.cc/" target="_blank">NeurIPS 2022</a>, <font color="firebrick">Oral</font> </b> <br>
				<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2202.01771" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We investigate the effectiveness of language modeling (LM) to scaffold learning and generalization in autonomous decision-making. We describe a framework for imitation learning in which goals and observations are represented as a sequence of embeddings, and translated into actions using a policy network initialized with a pre-trained transformer LM. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans.</p>
				</div>
			</div><hr>






			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		

			<br>
			<div id="others" style="padding-top: 80px; margin-top: -80px;">
				<h5>Others</h5>
			</div><br>



			<!-- 
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iclr-watch-and-help/multiagent.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</font></b><br>
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="https://www.tshu.io/" target="_blank">Tianmin Shu</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="" target="_blank">Zilin Wang</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, 
				<a href="https://www.cs.utoronto.ca/~fidler/" target="_blank">Sanja Fidler</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				<br>
				<b><a href="https://iclr.cc" target="_blank">ICLR 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://www.cooperativeai.com/neurips-2020" target="_blank">NeurIPS Cooperative AI Workshop 2020</a>, <font color="firebrick">Best Paper Award</font> </b> <br>
				<a href="http://virtual-home.org/watch_and_help/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2010.09890.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/xavierpuigf/watch_and_help" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=lrB4K2i8xPI&feature=youtu.be" target="_blank"> <small>[Video]</small></a>
				<a href="http://virtual-home.org/" target="_blank"> <small>[VirtualHome]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that the proposed challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale.</p>
				</div>
			</div><hr>
			-->


			

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-nerf-dy/nerf-dy.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<a href="https://people.csail.mit.edu/liyunzhu/" target="_blank">Yunzhu Li*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>, 
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				(*equal contribution)
                <br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a>, <font color="firebrick">Oral</font> </b> <br>
				<b><a href="https://rssvlrr.github.io/" target="_blank">RSS Visual Learning and Reasoning for Robotics Workshop 2021</a></b> <br>
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> We aim to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution.</p>
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iccv-hoi/teaser6.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>,
				<a href="https://www.di.ens.fr/~josef/" target="_blank">Josef Sivic</a>, and
				<a href="https://research.adobe.com/person/bryan-russell/" target="_blank">Bryan Russell</a>
				<br>
				<b><a href="http://iccv2021.thecvf.com/" target="_blank">ICCV 2021</a></b> <br>
				<a href="https://shuangli-project.github.io/weakly-supervised-human-object-detection-video/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2110.03562" target="_blank"> <small>[Paper]</small></a>
				<a href="https://shuangli-project.github.io/VHICO-Dataset/" target="_blank"> <small>[V-HICO Dataset]</small></a>
				<a href="https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video" target="_blank"> <small>[Code]</small></a>
                <a href="https://www.youtube.com/watch?v=1Gw9qld6Vx8" target="_blank"> <small>[Video]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and the object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects as a form of self-supervision. It allows detecting rare and unseen human-object interactions in a zero-shot manner. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset.</p>
				</div>
			</div><hr>



		</script>


	</div>
	
	
	<br><br>




	<!-- Service -->
	<div class="container">
		<h3 id="" style="padding-top: 80px; margin-top: -80px;">More Works</h3>
		<ul>
			<li> <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">Energy-based Models</a></li>
		</ul>
	</div><br><br>






	
	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Massachusetts Institute of Technology 2022</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(2);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
