<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" rel="stylesheet">
	<script src="js/main.js"></script>
  <script src="js/scroll.js"></script>
</head>

<title>Generalization</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<a class="navbar-brand" href="#Home">Learning how to Generalize</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="http://accessibility.mit.edu" target="_blank">Accessibility</a>
				</li>
			</ul>
		</div>
	</nav>








	<!-- Publications -->
	<br><br><br><br>
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">Mission</h3>

		<p>A key aspect of human intelligence is the ability to learn increasingly complex concepts by synthesizing simple ideas, enabling both rapid learning and adaptation of knowledge. To unlock its full potential, AI must exhibit similar generalization abilities.</p>
			<!-- We aim to teach AI systems to exhibit such generalization ability in different domains.  -->

		<p>Our research into AI generalization explores two paradigms: 1) training high-capacity models on massive unstructured data to achieve generalization from data, and 2) learning explicit compositional representations to enable generalization by composing different concepts. We hope our study on generalization will provide a possible solution for future AI systems to solve a wide range of real-world tasks.</p>
		<!-- We hope our study on generalization will provide a possible solution for future AI systems to continually learn from the world by combining previously learned knowledge and new skills. -->
		

		
		<hr>
	</div>




	<!-- Publications -->
	<br><br>
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
 			<br>

			<small><small>
		  	<font color="black">Research Topics:
		  		<a href="#learningdata" onclick="showPubs(2)">Learning from massive unstructured data</a> /
		  		<a href="#learningexplicit" onclick="showPubs(2)">Learning explicit compositional representations</a>
			</font><br>
			</small></small>
		</h3>


		<div id="pubs"></div>



		<script id="pubs_by_topic" language="text">
		
			<hr>

			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		

			
			<div id="learningdata" style="padding-top: 80px; margin-top: -80px;">
				<h5>Learning from massive unstructured data</h5>
			</div>
			<br>



			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/21-virtualhome-language/vid1.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Pre-Trained Language Models for Interactive Decision-Making</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="" target="_blank">Chris Paxton</a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://clintonjwang.github.io/" target="_blank">Clinton Wang</a>,
				<a href="" target="_blank">Linxi Fan</a>,
				<a href="https://taochenshh.github.io" target="_blank">Tao Chen</a>,
				<a href="" target="_blank">De-An Huang</a>,
				<a href="" target="_blank">Ekin Aky√ºrek</a>, 
				<a href="" target="_blank">Anima Anandkumar<sup>+</sup></a>,
				<a href="" target="_blank">Jacob Andreas<sup>+</sup></a>,
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch<sup>+</sup></a>, 
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba<sup>+</sup></a>, and
				<a href="" target="_blank">Yuke Zhu<sup>+</sup></a> <br>
				Junior authors are ordered based on their contributions and senior authors<sup>+</sup> are ordered alphabetically
				<br>
				<b><a href="" target="_blank">arXiv 2022</a></b> <br>
				<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2202.01771" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We investigate the effectiveness of language modeling (LM) to scaffold learning and generalization in autonomous decision-making. We describe a framework for imitation learning in which goals and observations are represented as a sequence of embeddings, and translated into actions using a policy network initialized with a pre-trained transformer LM. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans.</p>
				</div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iclr-watch-and-help/multiagent.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</font></b><br>
				<a href="https://people.csail.mit.edu/xavierpuig/" target="_blank">Xavier Puig</a>,
				<a href="https://www.tshu.io/" target="_blank">Tianmin Shu</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="" target="_blank">Zilin Wang</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, 
				<a href="https://www.cs.utoronto.ca/~fidler/" target="_blank">Sanja Fidler</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				<br>
				<b><a href="https://iclr.cc" target="_blank">ICLR 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://www.cooperativeai.com/neurips-2020" target="_blank">NeurIPS Cooperative AI Workshop 2020</a>, <font color="firebrick">Best Paper Award</font> </b> <br>
				<a href="http://virtual-home.org/watch_and_help/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2010.09890.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/xavierpuigf/watch_and_help" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=lrB4K2i8xPI&feature=youtu.be" target="_blank"> <small>[Video]</small></a>
				<a href="http://virtual-home.org/" target="_blank"> <small>[VirtualHome]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that the proposed challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale.</p>
				</div>
			</div><hr>





			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
			<!---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------->
		

			<br>
			<div id="learningexplicit" style="padding-top: 80px; margin-top: -80px;">
				<h5>Learning explicit compositional representations</h5>
			</div><br>




			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
                		<source src="files/paper/22-compose-duffusion-model/teaser_glide.mp4" type="video/mp4">
            		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Composable Diffusion Models</font></b><br>
				<a href="https://nanliu.io/" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li*</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>
				(*equal contribution)
				<br>
				<b><a href="" target="_blank">arXiv 2022</a></b> <br>
				<a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2206.01714.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch" target="_blank"> <small>[Github]</small></a>
				<a href="https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/compose_glide.ipynb" target="_blank"> <small>[Colab]</small></a>
				<a href="https://huggingface.co/spaces/Shuang59/Composable-Diffusion" target="_blank"> <small>[Demo]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p>We interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation.</p>
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
	            		<source src="files/paper/21-neurips-compose-relation/clevr_teaser.mp4" type="video/mp4">
	        		</video>
				</div>
				<div class="col-md-9">
				<b><font color="black">Learning to Compose Visual Relations</font></b><br>
				<a href="" target="_blank">Nan Liu*</a>,
				<a href="https://people.csail.mit.edu/lishuang/">Shuang Li*</a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du*</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>
				(*equal contribution)
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2021</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<b><a href="https://ctrlgenworkshop.github.io/accepted_papers.html" target="_blank">NeurIPS Workshop on Controllable Generative Modeling 2021</a>, <font color="firebrick">Outstanding Paper Award</font> </b> <br>
				<a> Press coverage: <a href="https://news.mit.edu/2021/ai-object-relationships-image-generation-1129" target="_blank">MIT News</a>, <a href="https://www.csail.mit.edu/news/artificial-intelligence-understands-object-relationships" target="_blank">MIT CSAIL News</a> </a> <br>
				<a href="https://composevisualrelations.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2111.09297" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/nanlliu/compose-visual-relations" target="_blank"> <small>[Code]</small></a><br>
				</div>
				<div class="col-md-12">
				<br>
				<p>The visual world around us can be described as a structured set of objects and their associated relations. In this work, we propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decomposition enables our model to effectively understand the underlying relational scene structure.</p>
				</div>

			</div><hr>



			<div class="row">
				<div class="col-md-3">
				<img class="img-fluid img-rounded" src="files/paper/21-neurips-decompose/teaser.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Unsupervised Learning of Compositional Energy Concepts</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>,
				<a href="https://www.yash-sharma.com" target="_blank">Yash Sharma</a>,
				<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2021</a></b> <br>
				<a href="https://energy-based-model.github.io/comet/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2111.03042.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yilundu/comet" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> We introduce an approach to decompose images, in an unsupervised manner, into separate component energy functions. 
	            These energy functions can both represent global factors of variation, such as facial expression and hair color, as well
	            as local factors of variations, such as the objects in a scene. Decomposed energy functions generalize well, and may be
	            recombined with energy function discovered by training a separate instance of approach on a different dataset, enabling
	            the recombination of objects and lighting conditions across datasets.
				</div>
			</div><hr>



			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/20-neurips-ebm-compositional/comp-face.gif" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Compositional Visual Generation with Energy Based Models</font></b><br>
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>, and
				<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en" target="_blank">Igor Mordatch</a>
				<br>
				<b><a href="https://neurips.cc/" target="_blank">NeurIPS 2020</a>, <font color="firebrick">Spotlight</font> </b> <br>
				<a href="https://energy-based-model.github.io/compositional-generation-inference/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/pdf/2004.06030.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yilundu/ebm_compositionality" target="_blank"> <small>[Code]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given one distribution for smiling face images, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We showcase the breadth of unique capabilities of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image. 
				</div>
			</div><hr>




			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="files/paper/21-iccv-hoi/teaser6.png" style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<b><font color="black">Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions</font></b><br>
				<a href="https://people.csail.mit.edu/lishuang/"><u><b>Shuang Li</b></u></a>,
				<a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>,
				<a href="https://www.di.ens.fr/~josef/" target="_blank">Josef Sivic</a>, and
				<a href="https://research.adobe.com/person/bryan-russell/" target="_blank">Bryan Russell</a>
				<br>
				<b><a href="http://iccv2021.thecvf.com/" target="_blank">ICCV 2021</a></b> <br>
				<a href="https://shuangli-project.github.io/weakly-supervised-human-object-detection-video/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2110.03562" target="_blank"> <small>[Paper]</small></a>
				<a href="https://shuangli-project.github.io/VHICO-Dataset/" target="_blank"> <small>[V-HICO Dataset]</small></a>
				<a href="https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video" target="_blank"> <small>[Code]</small></a>
                <a href="https://www.youtube.com/watch?v=1Gw9qld6Vx8" target="_blank"> <small>[Video]</small></a>
				</div>
				<div class="col-md-12">
				<br>
				<p> We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and the object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects as a form of self-supervision. It allows detecting rare and unseen human-object interactions in a zero-shot manner. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset. 
				</div>
			</div><hr>


	
		</script>


	</div>
	
	
	<br><br>




	<!-- Service -->
	<div class="container">
		<h3 id="" style="padding-top: 80px; margin-top: -80px;">More Works</h3>
		<ul>
			<li> <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">Energy-based Models</a></li>
		</ul>
	</div><br><br>





	<!-- Service -->
	<div class="container">
		<h3 id="Contact" style="padding-top: 80px; margin-top: -80px;">Contact</h3>
		<ul>
			<li> <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>: lishuang@mit.edu </li>
			<li> <a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>: yilundu@mit.edu</li>
		</ul>
	</div><br><br>





	
	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; Massachusetts Institute of Technology 2022</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(2);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
